% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rvm.R
\name{rvm}
\alias{rvm}
\title{Relevant Vector Machine}
\usage{
rvm(
  X,
  y,
  max_basis = 1000,
  qlscale = c(0.2, 0.5),
  lscale = NULL,
  lscale_probs = NULL,
  prune_thresh = 1e+06,
  drop_models = TRUE,
  tol = 0.005,
  maxiter = 2000,
  mc_cores = 1,
  verbose = TRUE
)
}
\arguments{
\item{X}{A dataframe or matrix of predictors scaled to be between 0 and 1}

\item{y}{a reponse vector of length n}

\item{max_basis}{Bounds complexity by performing LASSO on initial basis functions when ncol(X) exceeds max_basis.}

\item{qlscale}{Discrete lengthscale set on quantile scale (quantiles of the distribution of pairwise X distances).}

\item{lscale}{Discrete lengthscale set. Overrides qlscale when specified.}

\item{lscale_probs}{Prior probabilities corresponding to the lengthscale set.}

\item{prune_thresh}{When alpha_i exceed prune_thresh, we effectively set it to infinity speeding up future matrix inverse solves.}

\item{drop_models}{Should models (corresponding to lscale) be dropped if they have sufficiently small posterior probability?}

\item{tol}{Tolerance for early stopping of hyperparameter optimization.}

\item{maxiter}{Number of iterations of EM algorithm before stopping.}

\item{mc_cores}{How many cores to use (for parallelizing over various lengthscales)}

\item{verbose}{Should progress be printed?}
}
\description{
A Bayesian RVM implementation (Tipping 1999). This implementation exists only because the (much more efficient) kernlab implementation doesn't provide probabilistic predictions (or the capability to do so).
}
\details{
Algorithm has complexity O(nm^2). Candidate points are greedily selected to maximize a scoring criterion (Eq. 8 in Keerthi & Chu), conditional on kernel parameters. Once a subset is obtained, the GPfit package is used to estimate kernel parameters and the process repeats.
}
\examples{
X <- lhs::maximinLHS(100, 2)
f <- function(x) 10.391*((x[1]-0.4)*(x[2]-0.6) + 0.36)
y <- apply(X, 1, f) + stats::rnorm(100, 0, 0.1)
fit <- rvm(X, y)
}
\references{
Tipping, Michael. "The relevance vector machine." Advances in neural information processing systems 12 (1999).
}
